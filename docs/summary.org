#+TITLE: Machine learning convective parametrization
#+AUTHOR: Noah D. Brenowitz and Cristopher Bretherton

* Problem description
  
The goal of convective parametrization is to close the large scale budgets of
humidity and temperature. This closure can be either stochastic or deterministic

Given the entropy equation for the coarse-resolution model

\[
s_t + V_s + H_s = - R  + G 
\]

\[
q_{v,t} + V_q + H_q = - C 
\]
I have chosen not use the derivatives above, because the $V$ and $H$ terms are meant to explicitly represent the coarse-resolution models approximation.

A convective parametrization predicts 

\[C(x,y,t) \approx\tilde{C}(\{q_v(x,y,z) \forall z\} + \{s(x,y,z) \forall z\})\]

   
* Training Data

* Problem with discrete time

  For a autonomous simple dynamical system given by $\dot{x} = f(x)$, theory shows that if we an obtain a good approximation of $f$, than we can efficiently sample the dynamics. However, there are several problems for us:

  1. We do not observe $f$, but rather $f$ evaluated on a discrete set of time points
  2. Our problem is not autonomous or deterministic
 

  For these reasons, we cannot simply find a neural network approximation to the discrete time derivative like so:
  \[ \frac{x^{n+1} -x^{n}}{\Delta t} - G_n = f_{NN}(x^n) \]


* Multiple time step objective function

  We found that fitting a function for the apparent heat source and moisture sink computed using derivatives leads to an unstable scheme.

  To fix this, we change our objective function to demand that the actual simulated time course using the neural network convective scheme match the observed temperature and humidity profiles. This heavily penalizes consistent but unstable approximations.

  Specifically, this objective function is given by

  $$ J(f) =  \sum_{i,n} \sum_{m=0}^{\ell} ||f^m(x_i^{n}) - x_i^{n+m}||^2_w,$$

  where $f^m$ is the time stepping operator defined recursively by
  $$f^m(x) = f(f^{m-1}(x)) + \frac{\Delta t}{2}\left(g^{m-1} + g^{m}\right).$$

