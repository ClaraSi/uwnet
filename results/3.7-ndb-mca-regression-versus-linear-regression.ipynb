{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I compare the performance of two linear models for $Q_{1c}$ and $Q_2$ which have units K/day. In both cases the inputs are the concatenated profiles of $s_l$ (g/kg) and $q_T$(K). This produces a matrix $X$ of iunputs, where each row is a 68 dimensional vector for a given horizontal location, for a given time point. Concatenating $Q_{1c}$ and $Q_2$ produces a matrix of outputs, $Y$, with 68 columns. For the tropics region of NGAqua, both $X$ and $Y$ have 821248 rows.\n",
    "\n",
    "We will call the first linear regression, **LR**. It consists of the following steps\n",
    "1. Discard any column which has a variance of less then .001. This removes the columns corresponding to the upper atmospheric moisture field, which is essentially 0. Removing these columns is necessary, to avoid an ill-conditioned linear regression.\n",
    "2. Perform linear least squares regression (w/ constant profile added).\n",
    "\n",
    "The next linear model we fit which we call **MCR(n)**, prefilters the inputs $X$ by projecting onto the first $n$ MCA modes. It consists of these steps:\n",
    "1. Normalize each column by removing its mean, and dividing by the mass-weighted vertical average of the standard deviation of the corresponding physical variable $q_T$, $s_L$.\n",
    "2. Weight each column by the square root of the layer mass.\n",
    "3. Peform MCA analysis with $n$ modes, and compute the matrix $P_n$ which projects $X$ onto this modes $n$ modes.\n",
    "4. Project the inputs onto the MCA mode.\n",
    "5. Perform linear least squares regression (w/ constant profile) with the outputs of the previous step (a $821248\\times n$ matrix) onto the full outputs $Y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from lib.models import get_linear_model, get_mca_mod\n",
    "from lib.util import weighted_r2_score\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and get the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"../data/ml/ngaqua/data.pkl\")\n",
    "\n",
    "_, weight_out = data['w']\n",
    "\n",
    "x_train, y_train = data['train']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "\n",
    "# get objects for fitting linear and MCA models\n",
    "lm = get_linear_model(data)\n",
    "mcr  = get_mca_mod(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which computes the mass weighted R2 of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(mod, x, y):\n",
    "    pred = mod.predict(x)\n",
    "    return weighted_r2_score(y, pred, weight_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 of the linear model is 28%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(x_train, y_train)\n",
    "print(\"LM Cross Validation R2 = \",\n",
    "      score_model(lm, x_test, y_test),\n",
    "     \"Training R2 = \", score_model(lm, x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the R2 of the maximum component regression for different numbers of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1,2,5,10,20,30,40,50,60,68]:\n",
    "    # set the number of components to keep\n",
    "    mcr.mca.set_params(n_components=n)\n",
    "    mcr.fit(x_train, y_train)\n",
    "    input_var_explained = mcr.mca.explained_var_\n",
    "    print(f\"MCR(n_comp={n})\\n\",\n",
    "          \"\\n % Input variance explained\", input_var_explained,\n",
    "          \"\\n Training R2\", score_model(mcr, x_train, y_train),\n",
    "          \"\\n Cross Validation R2 = \", score_model(mcr, x_test, y_test),\"\\n\"*2\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we retain more components, the fraction of the input variance which is explained by MCA goes from 0 to 1. Surprisingly the R2 computed on the training and testing data does not increase quickly at all. For example, the model with 20 MCA modes explains .98 of the input variance but only .20 of the output variance. **MCR** achieves its maximum cross validation performance when at least 50 modes are kept.\n",
    "\n",
    "We can interpret this in two ways:\n",
    "\n",
    "1. Modes which account for only 2% of the variance are extremely important\n",
    "2. The relationship between the input modes and output modes is highly nonlinear, and adding more degrees of freedom helps ameliorates this nonlinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strongly Nonlinear response of convection to the MCA modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Maybe we can learn something by looking at scatter plots of the **MCR(2)** model's predictions versus the actual ouputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.plots.model_evaluation import scatter_plot_z\n",
    "from lib.util import dict_to_xr, output_to_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcr.mca.set_params(n_components=2)\n",
    "mcr.fit(x_train, y_train)\n",
    "\n",
    "pred= output_to_xr(mcr.predict(x_test), y_test.coords)\n",
    "truth= output_to_xr(y_test, y_test.coords)\n",
    "lr = output_to_xr(lm.predict(x_test), y_test.coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = dict_to_xr({'MCR(2)': pred, 'truth': truth, \"LR\": lr})\n",
    "scatter_plot_z(plot_data.Q1c, \"truth\", [\"MCR(2)\", \"LR\"], \"variable\", engine='points')\\\n",
    ".redim.values(z=[6555])\\\n",
    ".layout(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for a z=6555 m, the relationship between the MCR(2) prediction of $Q_{1c}$ and the actual output of $Q_{1c}$ is much more nonlinear than for the LR prediction, but it seems we should still be able to fit this well using a nonlinear model. This fits with the notion that small dimensional nonlinear systems, can be embeded with higher dimension nonlinear spaces, and it is a well known fact that linear model perform better in high dimensional spaces. It also makes sense in terms of physical quantities because convection probably responds more nonlinearly to column water vapor, than to the humidity at some height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single layer perceptron on the MCA scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the a nonlinear model using the first two MCA modes as an input performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.torch_models import TorchRegressor, single_layer_perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import MSELoss\n",
    "mse = MSELoss()\n",
    "w = Variable(torch.FloatTensor(weight_out))\n",
    "\n",
    "def loss_function(output, y):\n",
    "    return mse(output.mul(w.sqrt()), y.mul(w.sqrt()))\n",
    "\n",
    "slp = TorchRegressor(single_layer_perceptron, loss_fn=loss_function,\n",
    "                     optim_kwargs=dict(lr=.002),\n",
    "                    num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_mlp = get_mca_mod(data, mod=make_pipeline(StandardScaler(), slp))\n",
    "\n",
    "# only train on a subset of the data\n",
    "inds = np.random.choice(x_train.shape[0], 100000, replace=False)\n",
    "\n",
    "mca_mlp.fit(x_train.data[inds], y_train.data[inds])\n",
    "mca_mlp_pred = output_to_xr(mca_mlp.predict(x_test), y_test.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the R2 value of the nonlinear regression on top of MCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve[invert_axes=True]\n",
    "def r2_to_xr(truth, pred):\n",
    "\n",
    "    sse = ((pred - truth)**2).sum(['x', 'y', 'time'])\n",
    "    ss = ((truth.mean(['x','y','time']) - truth)**2).sum(['x', 'y', 'time'])\n",
    "    return 1 - sse/ss\n",
    "\n",
    "r2_plot_data = dict_to_xr({\n",
    "    'mca_mlp': r2_to_xr(truth.Q1c, mca_mlp_pred.Q1c),\n",
    "    'LR': r2_to_xr(truth.Q1c, lr.Q1c),\n",
    "    'MCR': r2_to_xr(truth.Q1c, pred.Q1c)\n",
    "}, dim_name=\"model\")\n",
    "\n",
    "\n",
    "hv.Dataset(r2_plot_data).to.curve(\"z\").overlay(\"model\").redim.label(Q1c=\"R2 of Q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the prediction performs much better in the middle of the atmosphere than, and make large errors below 2000m and in the stratosphere.\n",
    "\n",
    "Also, the nonlinear MCA_MLR regression performs nearly as well as the linear model, which has many more inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the scatter plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_data = dict_to_xr({'MCR(2)_SLP':mca_mlp_pred, 'MCR(2)': pred, 'truth': truth, \"LR\": lr})\n",
    "scatter_plot_z(plot_data.Q1c, \"truth\", ['MCR(2)_SLP', \"MCR(2)\", \"LR\"], \"variable\", engine='points')\\\n",
    ".layout(\"model\").cols(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
