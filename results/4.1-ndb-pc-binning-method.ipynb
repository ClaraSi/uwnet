{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will try the principal component binning approach to convective parametrization. This approach gives rise to a natural stochastic scheme, but for the time-being, I will restrict my focus to how well the scheme describes then mean. If it cannot capture the mean, then the scheme is not doing a good job of capture the distribution of Q1 and Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"../data/ml/ngaqua/data.pkl\")\n",
    "ntrain = 10000\n",
    "\n",
    "scale_in, scale_out = data['scale']\n",
    "weight_in, weight_out = data['w']\n",
    "x_train, y_train = data['train']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "p = xr.open_dataset(\"../data/raw/ngaqua/stat.nc\").p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the mean performs. Not the well obviously. It seems have special difficulty capturing the second baroclinic mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# make a transformer which scales the data\n",
    "input_scaler = FunctionTransformer(lambda x: x* np.sqrt(weight_in)/scale_in)\n",
    "\n",
    "\n",
    "from lib.util import output_to_xr, dict_to_xr, swap_coord\n",
    "\n",
    "\n",
    "def plot_preds(predictions):\n",
    "    predictions = {k: output_to_xr(v, y_test.coords)\n",
    "                   for k, v in predictions.items()}\n",
    "\n",
    "    preds_xr = dict_to_xr(predictions, dim_name=\"model\")\\\n",
    "    .pipe(lambda x: swap_coord(x, z=p))\n",
    "    height = len(predictions) * 2\n",
    "    axs = preds_xr.isel(x=0, y=8).Q1c.plot(col='model', col_wrap=1, cmap=\"inferno\", vmin=-20, vmax=100,\n",
    "                                           figsize=(8,height))\n",
    "    plt.gca().invert_yaxis()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = make_pipeline(input_scaler, PCA(n_components=2, whiten=True))\n",
    "\n",
    "mod = make_pipeline(preprocessor, KNeighborsRegressor(20))\n",
    "mod.fit(x_train, y_train)\n",
    "\n",
    "y_pred = mod.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = {'true': y_test, 'pca(2) | knn(n=20)': y_pred}\n",
    "    \n",
    "plot_preds(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean of the knearest neighbors method does not perform particularly well, and mostly just captures the first baroclinic heating mode. what is its R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models import weighted_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_r2_score(y_test, y_pred, weight=weight_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this performs much worse the neural network or even the linear model. The R2 score on the testing data is negative. This is probably because the predicted heating is much too smooth in time. Maybe a binning approach will work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning approach\n",
    "\n",
    "First we need to decise what bins we should be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scores = preprocessor.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(x_scores[:,0], x_scores[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hexbin plot shows that we should maybe lay down a uniform grid with size 0.1 in PC1 and PC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from toolz import valmap\n",
    "from functools import reduce\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class Binner2D(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Regressor which returns the average value over a grid in the input features\n",
    "    \n",
    "    This is very similar to a nearest neighbors lookup in theory.\n",
    "    \n",
    "    If input data is given which falls in a bin never observed in the training dataset,\n",
    "    the average over all values in the bin of the first column is given.\n",
    "    \"\"\"\n",
    "    \n",
    "    def columns_bins(self, x):\n",
    "        xmin, xmax = x.min(), x.max()\n",
    "#         this failed\n",
    "#         return np.linspace(xmin, xmax, 200)    \n",
    "\n",
    "        return np.percentile(x, np.arange(.5,100,.5))\n",
    "        \n",
    "    def get_bin_membership(self, x):\n",
    "        return np.hstack(np.digitize(x[:,i], self.bin_grids_[i])[:,None]\n",
    "                         for i in range(x.shape[1]))\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        x, y = np.asarray(x), np.asarray(y)\n",
    "        self.bin_grids_ = [self.columns_bins(x[:,i]) for i in range(x.shape[1])]\n",
    "        \n",
    "        self.bin_membership_ = self.get_bin_membership(x)\n",
    "        \n",
    "        # store indexes in a dict\n",
    "        # this provides very fast lookup\n",
    "        self.bins_ = defaultdict(list)\n",
    "        for i in range(x.shape[0]):\n",
    "            memb = tuple(self.bin_membership_[i].flat)\n",
    "            self.bins_[memb].append(i)\n",
    "            \n",
    "            \n",
    "        # take the mean of the output in each bin\n",
    "        self.bin_out_avg_ = valmap(lambda inds: y[inds].mean(axis=0), self.bins_)\n",
    "        self.bin_counts_ = valmap(len, self.bins_)\n",
    "        \n",
    "        \n",
    "        # Take average over PC1\n",
    "        # This will be used as the output for the empty bins\n",
    "        pc1_avg = {}\n",
    "        for i,_ in enumerate(self.bin_grids_[0]):\n",
    "            non_empty_bins = []\n",
    "            for k, v in self.bins_.items():\n",
    "                if k[0] == i:\n",
    "                    non_empty_bins.extend(v)\n",
    "                \n",
    "            pc1_avg[i] = y[non_empty_bins].mean(axis=0)\n",
    "            \n",
    "            \n",
    "        self.pc1_avg = pc1_avg\n",
    "             \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def query_avg(self, memb):\n",
    "        memb = tuple(memb.flat)\n",
    "        \n",
    "        if memb in self.bin_out_avg_:\n",
    "            return self.bin_out_avg_[memb]\n",
    "        else:\n",
    "            return self.pc1_avg[memb[0]]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        membership = self.get_bin_membership(x)\n",
    "     \n",
    "        return np.vstack(self.query_avg(memb)[None,:] for memb in membership)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin2d = make_pipeline(input_scaler, PCA(n_components=2), Binner2D())\n",
    "bin2d.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['pca(2) | bin2d'] = bin2d.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the bin averaging gives very similar results to k nearest neighbors. Of the two, the k nearest neighbors is simpler to implement. It is also easier to use k nearest neighbors for data outside of the training dataset, because it will always return the nearest neighbors. On the other hand, the binning approach requires more adhoc approaches when the testing data is far from the training data.\n",
    "\n",
    "In summary, it is not clear if this method failed because the principal component basis is not good, and the performance could improve when using an improved basis for the inputs (e.g. MCA). Let's see if this is the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCA KNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.mca import MCARegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_scale = [np.sqrt(w)/sc for w, sc in zip(data['w'], data['scale'])]\n",
    "\n",
    "\n",
    "mod = make_pipeline(StandardScaler(), KNeighborsRegressor(20))\n",
    "mca_knn = MCARegression(mod=mod, scale=mca_scale, n_components=2)\n",
    "mca_knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This class uses far too much memory to process all the testing data in one go, so I make the predictions for different slices, and then concatenate these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_slices(n, k):\n",
    "    \"\"\"python generator for splitting an array into chunks of size k\"\"\"\n",
    "\n",
    "    starts = range(0, n, k)\n",
    "\n",
    "    for start in starts:\n",
    "        end = min(start + k, n)\n",
    "        yield slice(start, end)\n",
    "\n",
    "\n",
    "\n",
    "n = x_test.shape[0]\n",
    "output = np.vstack(mca_knn.predict(x_test[sl]) for sl in split_slices(n, 1000))\n",
    "\n",
    "predictions['mca(2) | knn(20)'] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the R2 of the MCA scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_r2_score(y_test, output, weight=weight_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, at least the quantity is positive, but still nowhere near as good as the Neural networks, which were getting a cross validation R2 of around 0.4-0.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted heating wiis much more concentrated over the times when the heating is actualy positive. Overall none of these techinques perform well when using the bin average or K nearest neighbors approach, so I am not hopeful that the averages will be much better. All in all, I think the single layer perceptron is the most viable model. Perhaps we could describe the stochasticity of the residual from that model using an approach like the one here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "1. The first 2 MCA modes give much better predictive value then the first 2 PCA modes\n",
    "2. KNearestNeighbors and two-dimensional binning give similar answers.\n",
    "3. These non-parameteric (i.e. lookup table like methods) perform substantially worse than the parametric neural network approaches, and are more during the prediction phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uw-machine-learning]",
   "language": "python",
   "name": "conda-env-uw-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
