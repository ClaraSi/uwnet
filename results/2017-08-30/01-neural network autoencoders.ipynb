{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = xr.open_dataset(\"wd/ml/X.nc\").isel(x=0)\n",
    "rho = xr.open_dataset(\"wd/stat.nc\").RHO[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnl.data_matrix import DataMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataMatrix(['z'], ['time'], ['qt'])\n",
    "X = dm.dataset_to_mat(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= X.mean(axis=1)[:,None]\n",
    "D = np.diag(np.sqrt(rho))\n",
    "# D = np.eye(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw = X@ np.sqrt(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating SVD using neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple keras hidden layer autoencoder with MSE loss function. This should return the same output as EOFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Sequential()\n",
    "\n",
    "\n",
    "nhidden = 4\n",
    "mod.add(Dense(nhidden, input_dim=X.shape[1], kernel_regularizer=regularizers.l2(0.0)))\n",
    "# mod.add(Activation('relu'))\n",
    "mod.add(Dense(X.shape[1], input_dim=nhidden))\n",
    "\n",
    "mod.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "mod.fit(Xw, Xw, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "l1 = mod.layers[0]\n",
    "\n",
    "coef = K.get_value(l1.kernel)\n",
    "\n",
    "plt.pcolormesh(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.evaluate(Xw, Xw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this doesn't seem to work very well. In theory this sample should work very well, since it is a convex problem. Maybe neural networks should be mostly used for classification type problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA(n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('pca', PCA(n_components=20)), ('reg', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['pca'].set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 600\n",
    "\n",
    "def score(pipe, X, Y):\n",
    "    \n",
    "    Yt = pipe.predict(X)\n",
    "    return 1 - np.mean((Yt-Y)**2)/Y.std()**2\n",
    "\n",
    "def train(n=4, train=600):\n",
    "    pipe.named_steps['pca'].set_params(n_components=n) \n",
    "    pipe.fit(Xw[:train], Xw[1:train+1])\n",
    "    return score(pipe, Xw[:train], Xw[1:train+1]), score(pipe, Xw[train-1:-1], Xw[train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_c = np.arange(1,64)\n",
    "\n",
    "train_error, test_error = zip(*[train(n) for n in range(1,64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_error, label='training score')\n",
    "plt.plot(test_error, label='testing score')\n",
    "plt.legend()\n",
    "plt.ylim([0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nst = n_c[np.argmax(test_error)]\n",
    "\n",
    "_, err  = train(n_c[np.argmax(test_error)])\n",
    "print(f\"Best test R2 of {err} with {nst} components\")\n",
    "\n",
    "plt.pcolormesh(pipe.named_steps['pca'].components_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
