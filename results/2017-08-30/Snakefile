import numpy as np
from scipy.interpolate import interp1d
import xarray as xr
from gnl.xarray import map_overlap
import dask.array as da
from dask.array.core import atop
from dask import delayed

configfile: "config.yaml"
include: "sam.rules"



def get_jupyter_inputs(wildcards):
    import json
    ipynb = wildcards.f + ".ipynb"
    with open(ipynb) as f:
        notebook = json.load(f)

    inputs = []
    for cell in  notebook["cells"]:
        inputs += cell.get("metadata", {})\
                      .get("inputs", [])

    inputs += notebook['metadata'].get('snakemake_inputs', [])

    return list(set(inputs))


rule all:
    input: "01-neural-network-autoencoders.done",
        "02-ridge-regression-cross-validation.done",
        "03-arcsinh-q1.done",
        "04-performance-linear-models.done"


@wrap_xarray_calculation
def source(x, f):
    dt = float(x.time[1] - x.time[0])
    return x.shift(time=-1) - dt/2 * (f.shift(time=-1) + f)

def mysel(x):
    return x.sel(time=slice(20,None)).isel(x=0)

rule inputs:
    input: "calc/qt.nc", "calc/sl.nc", "A64/2d/LHF.nc", "A64/2d/SHF.nc"
    output: "ml/X.nc"
    run:
        xr.merge((xopena(f) for f in input), join='inner')\
          .pipe(mysel)\
          .to_netcdf(output[0])

rule outputs:
    input: q2="budget/calc/qt.nc", q1="budget/calc/sl.nc"
    output: Y="ml/Y.nc"
    run:
        data_vars = dict(q1=xopena(input.q1),
                         q2=xopena(input.q2) * Lc / cp * 1e-3)

        xr.Dataset(data_vars).pipe(mysel).to_netcdf(output.Y)
rule ml:
    input: X="ml/X.nc", Y="ml/Y.nc", weight="w.nc"
    output: X="ml/x.npz", Y="ml/y.npz"
    script: "ml.py"


rule test_train_split:
    input: "{f}.nc"
    output: train="train/{f}.nc", test="test/{f}.nc"
    params: t_split=70.0
    script: "test_train_split.py"


rule reports:
    input:  get_jupyter_inputs
    output: touch("{f}.done")
    shell: "jupyter nbconvert --ExecutePreprocessor.timeout=600 --execute --to notebook --inplace {wildcards.f}.ipynb"
