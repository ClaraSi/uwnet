{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we saw that the Jacobian of the predicted heating had extremely bad performance. Including the upper atmospheric humidity (which has nearly 0 variance) caused this ill-conditioning just as it did for the linear response functions. In this notebook, I experiment with removing these null-variance features from the inputs to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from scipy.linalg import eigvals\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from lib.models.torch_models import predict, jacobian, train_euler_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from lib.hvops import quadmesh\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "invert_opts = dict(plot=dict(invert_yaxis=True, invert_axes=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/ml/ngaqua/time_series_data.npz\")\n",
    "\n",
    "\n",
    "X = data['X']\n",
    "G = data['G']\n",
    "scale = data['scales']\n",
    "w = data['w']\n",
    "\n",
    "# # we need to grap the pressure field from a different path\n",
    "p = xr.open_dataset(\"../data/raw/ngaqua/stat.nc\").p.values\n",
    "# t = dt * np.arange(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.apply_over_axes(np.mean, X, axes=(0,1,2)).ravel()\n",
    "mu = mu[:-14]\n",
    "\n",
    "\n",
    "sig = np.apply_over_axes(np.std, X, axes=(0,1,2)).ravel()\n",
    "sig = sig[:-14]\n",
    "\n",
    "x_mean = np.apply_over_axes(np.mean, X, axes=(0,1,2)).reshape((2,-1))\n",
    "x_std = np.apply_over_axes(np.std, X, axes=(0,1,2)).reshape((2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find highlight the 200 hPA level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points(color='red')\n",
    "\n",
    "ind = np.searchsorted(p[::-1], 200)\n",
    "nz = p.shape[0]\n",
    "print(\"200hPA index is\", ind)\n",
    "\n",
    "\n",
    "lay = hv.Curve((p, x_std[1,:])).opts(**invert_opts) * hv.Points((p[-ind], x_std[1,-ind]))\n",
    "lay.redim.label(y=\"std(qt) (g/kg)\", x=\"p (hPa)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we should not use the moisture above 200 hPa as an input to the algorithm. Let's train a neural network excluding these points. We only train with a random subset of 100000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper = train_euler_network(data, n=2, nsteps=1, ntrain=100000, weight_decay=0.0, nhidden=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the linear response function  $M(x)=\\log(I + h J(x))$, about the mean profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf= stepper.linear_response_function(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrf_plots(p, sig, jac, nz=34):\n",
    "    p_t = p\n",
    "    # q has fewer grid points\n",
    "    p_q = p[:-14]\n",
    "    \n",
    "    opts =\"\"\"\n",
    "    QuadMesh[invert_yaxis=True, invert_xaxis=True, colorbar=True](cmap='viridis') {+axiswise}\n",
    "    VLine(color='red')\n",
    "    \"\"\"\n",
    "    \n",
    "    jac = jac * sig\n",
    "    \n",
    "    # this holomap plots the panes of the LRF\n",
    "    holomap = hv.HoloMap(kdims=['d', 'f'])\n",
    "    holomap['sl', 'sl'] = quadmesh(p_t, p_t, jac[:nz,:nz])\n",
    "    holomap['sl', 'qt'] = quadmesh(p_q, p_t, jac[:nz,nz:])\n",
    "    holomap['qt', 'sl'] = quadmesh(p_t, p_q, jac[nz:,:nz])\n",
    "    holomap['qt', 'qt'] = quadmesh(p_q, p_q, jac[nz:,nz:])\n",
    "    holomap = holomap.redim.label(x=\"p (in)\", y=\"p (out)\")\n",
    "    \n",
    "    # cannot render ndlayout in layout\n",
    "    lrf=holomap['sl', 'sl'] + holomap['sl', 'qt'] + holomap['qt', 'sl']  + holomap['qt', 'qt']\n",
    "    \n",
    "    # now let's look at the eigenvalues\n",
    "    lam = eigvals(jac)\n",
    "    eigpts = hv.Points((lam.real, lam.imag), kdims=[r'$\\Re$', r'$\\Im$'])\n",
    "\n",
    "    \n",
    "    return (lrf + eigpts * hv.VLine(0.0)).opts(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf_plots(p, sig, lrf).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this pane looks very similar to the results we had with the linear response function. However, there are many small positive eigenvalues. This might not necessarily be a problem because we can just add some small amount of damping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time stepping with neural networks\n",
    "\n",
    "The data we are using are only evaulated with a coarse sampling time step of 3 hours. On the other hand, we will probably use 10-20 minute time step for the coarse resolution model. This means that the dynamical model we are trying to fit is \n",
    "$$ x^i_{n+1} = \\underbrace{f(f(\\ldots f}_{\\text{m times}}(x^i_n))) + \\int_{t_n}^{t_{n+1}} g(x(t), t) dt$$ \n",
    "where $i$ is the horizontal spatial index, and $n$ is the time step. The number of times the function $f$ is applied is $m=\\frac{\\Delta t}{h}$ where $h$ is the GCMs time step, and $\\Delta t$ is the sampling interval of the stored output. The integral on the right represents the approximately known terms such as advection, and $f$ represents the unknown source terms.\n",
    "\n",
    "We solve a minimization problem to find $f$. This is given by \n",
    "$$\n",
    "\\min_{a} \\lim_{m \\rightarrow \\infty} \\sum_{i,n} ||x^{i}_{n+1} - F^{(m)} x^i_{n} - g_n^{i}||_W^2 \\quad \\text{s.t.}\\quad F^{(m)}(\\cdot) = \\underbrace{f(f(\\ldots f}_{\\text{m times}}(\\cdot))),\\ f(x) = x +  \\frac{ \\Delta t}{m} a(x).\n",
    "$$\n",
    "Intuitively, the forward operator $F^{(m)}$ is the result applying $m$ forward euler steps to the system $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try performing this fit fior $m=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper = train_euler_network(data, n=5, nsteps=10, ntrain=100000, weight_decay=0.00, nhidden=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = stepper.linear_response_function(mu)\n",
    "lrf_plots(p, sig, M).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two lessons to using this euler stepping approach. The first is that we can achieve a similar performance using the many fewer hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single column performance of time stepping schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:-1,8,0,:-14]\n",
    "xp = X[1:, 8,0,:-14]\n",
    "g = G[:-1,8,0,:-14]\n",
    "\n",
    "t = np.arange(x.shape[0])*.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_pred = predict(stepper, x)\n",
    "\n",
    "q1_truth = (xp-x)/(3/24)-g\n",
    "q1_pred = (xp_pred-x)/(3/24) - g\n",
    "\n",
    "# q1_pred = predict(stepper.rhs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=200\n",
    "%%opts QuadMesh[invert_yaxis=True, colorbar=True, aspect=3](cmap='inferno')\n",
    "\n",
    "hv.HoloMap({\n",
    "    'pred': quadmesh(t, p, q1_pred[:,:34].T, kdims=['t (d)', 'p (mb)']),\n",
    "    'truth':  quadmesh(t, p, q1_truth[:,:34].T, kdims=['t (d)', 'p (mb)'])\n",
    "}).layout().cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prognostic performance (with L2 regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I test the performance of neural network trained with only one time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper = train_euler_network(data, n=4, nsteps=1, ntrain=100000, weight_decay=0.01, nhidden=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = stepper.linear_response_function(mu)\n",
    "lrf_plots(p, sig, M).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some interpolators for the forcing as well as the observed time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing = interp1d(t, g, axis=0)\n",
    "actual = interp1d(t, x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_soln(f):\n",
    "    \n",
    "    y = odeint(f, x[0], t[:400])\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2, figsize=(8,5), sharey=True)\n",
    "    qt_levs = np.arange(11)*2.5\n",
    "\n",
    "\n",
    "    t_levs = np.arange(12)*25 + 275\n",
    "    t_im = axs[0,0].contourf(x[:,:34].T, levels=t_levs)\n",
    "    axs[0,1].contourf(y[:,:34].T, levels=t_levs)\n",
    "    q_im = axs[1,0].contourf(x[:,34:].T, levels=qt_levs)\n",
    "    axs[1,1].contourf(y[:,34:].T, levels=qt_levs)\n",
    "\n",
    "    plt.colorbar(t_im, ax=axs[0,:].tolist())\n",
    "    plt.colorbar(q_im, ax=axs[1,:].tolist())\n",
    "    \n",
    "    axs[0,1].set_title(\"Prediction\")\n",
    "    axs[0,0].set_title(\"Truth\")\n",
    "    \n",
    "    axs[0,0].set_ylabel('sl')\n",
    "    axs[1,0].set_ylabel('qt')\n",
    "   \n",
    "\n",
    "def f(x, t):\n",
    "    return predict(stepper.rhs, x) #- (x-mu)/4 + forcing(t)\n",
    "\n",
    "print_soln(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the solutions tend to explode behavior when the neural net is the only active source term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including some weak damping $\\tau=20\\text{ day}$ towards the mean stabilizes the scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, t):\n",
    "    return predict(stepper.rhs, x) - (x-mu)/20 #+ forcing(t)\n",
    "print_soln(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we add in the advection forcing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, t):\n",
    "    return predict(stepper.rhs, x) - (x-mu)/20 + forcing(t)\n",
    "print_soln(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unstable again, so we should probably increase the damping once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, t):\n",
    "    return predict(stepper.rhs, x) - (x-mu)/4 + forcing(t)\n",
    "print_soln(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also damp towards the actual time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, t):\n",
    "    return predict(stepper.rhs, x) - (x-actual(t))/4 + forcing(t)\n",
    "print_soln(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the answer is basically the as before. What happens if we turn off the neural network scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, t):\n",
    "    return - (x-actual(t))/4 + forcing(t)\n",
    "print_soln(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much worse prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
