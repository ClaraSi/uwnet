{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import SymLogNorm\n",
    "from matplotlib.mlab import griddata\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = joblib.Memory(\"../data/cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the transformed data from the mca analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_data = joblib.load(\"../data/ml/ngaqua/data.pkl\")\n",
    "# lm_data = joblib.load(\"../data/ml/ngaqua/linear_model.pkl\")\n",
    "mca_data = joblib.load(\"../data/ml/ngaqua/mca.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = mca_data['transformed']\n",
    "\n",
    "# scale these to have the same standard deviation\n",
    "x = StandardScaler().fit_transform(x)\n",
    "y = StandardScaler().fit_transform(y)\n",
    "\n",
    "\n",
    "n_samp, n_comp= x.shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the rows of these matrices corresponds to all horizontal-temporal samples of the data. Let's plot the various pairwise probability density functions of the input (QT, SL ,LHF, SHF) and output (Q1c, Q2) MCA modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_hexbin_matrix(fun, x, y,axs=None, **kwargs):\n",
    "    n_samp, n_comp_x = x.shape\n",
    "    n_comp_y  = y.shape[1]\n",
    "    \n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"x and y must have same number of rows\")\n",
    "    \n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(n_comp_y, n_comp_x, figsize=(2*n_comp_x, 2*n_comp_y), sharex=True, sharey=True)\n",
    "\n",
    "    axs.shape = (n_comp_y, n_comp_x)\n",
    "    \n",
    "    ims = []\n",
    "    for i in range(n_comp_y):\n",
    "        for j in range(n_comp_x):\n",
    "#             axs[i,j].scatter(x[idx_rand,i], y[idx_rand,j], **kwargs)\n",
    "            im = fun(axs[i,j], x[:,j], y[:,i])\n",
    "#             im = axs[i,j].hexbin(x[:,i], y[:,j], cmap='inferno_r')\n",
    "            axs[i,j].set_ylim([-3,3])\n",
    "            axs[i,j].set_xlim([-3,3])\n",
    "            \n",
    "            ims.append(im)\n",
    "            \n",
    "            if i == 0:\n",
    "                axs[i,j].set_title(f\"X Comp {j+1}\")\n",
    "            if i == n_comp_y:\n",
    "                axs[i,j].set_xlabel(f\"X Comp {j+1}\")\n",
    "            if j == 0:\n",
    "                axs[i,j].set_ylabel(f\"Y Comp {i+1}\")\n",
    "                \n",
    "    return axs, ims\n",
    "\n",
    "\n",
    "def myhexbin(ax, x, y):\n",
    "    return ax.hexbin(x, y, cmap='inferno_r')\n",
    "\n",
    "\n",
    "def myline(ax, x, y):\n",
    "    inds = x.argsort()\n",
    "    x, y = x[inds], y[inds]\n",
    "    \n",
    "    b,e = x[0], x[-1]\n",
    "    xg = np.linspace(b, e, 200)\n",
    "    \n",
    "    yg = np.interp(xg, x, y)\n",
    "    \n",
    "    return ax.plot(xg, yg)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs, ims = cross_hexbin_matrix(myhexbin, x, y)\n",
    "plt.tight_layout()\n",
    "plt.colorbar(ims[0], ax=list(axs.flat), fraction=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there is a significant nonlinear structure to the relationship between the input and output MCA components. This nonlinearity is particularly obvious when looking at the scatter plot between the first output MCA component and the various input components.\n",
    "\n",
    "In addition to this nonlinearity, the blobiness of these two dimensional plots shows that the data are quite noisy.\n",
    "\n",
    "How well can a linear model fit this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "for comp in range(4):\n",
    "    lm =  LinearRegression().fit(x,y[:,comp])\n",
    "    pred= lm.predict(x)\n",
    "    score = lm.score(x, y[:,comp])\n",
    "    print(f\"Component {comp+1} R2: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score of this data is not good at all. What do the pairwise pdf plots of $(x, \\tilde{y})$, where $\\tilde{y}$ is the predicted value of y, for the overall data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = LinearRegression().fit(x,y).predict(x)\n",
    "cross_hexbin_matrix(myhexbin, x, pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this plot looks almost completely different from the pdf plots above. For example, the top-left pane of both plots shows the joint distribution of the first input and output components. While the linear model can capture some curvature in these pairwise pdfs, it does not come close to appoximating the nonlinearity of the actual MCA components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear models for the first mode\n",
    "\n",
    "Let's focus our analysis on just predicting the first MCA mode given the four input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = y[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the R2 for a linear fit is pretty low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def plot_prediction(true, y):\n",
    "    \"\"\"Plot the prediction\"\"\"\n",
    "    plt.hexbin(y, true, cmap='inferno_r', norm=SymLogNorm(100))\n",
    "    plt.colorbar()\n",
    "    plt.plot((-3,3), (-3,3), 'k-')\n",
    "    plt.xlim([-3,3])\n",
    "    plt.ylim([-3,3])\n",
    "    \n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Observed')\n",
    "    \n",
    "def plot_performance(y_true, y_pred):\n",
    "#     pred = mod.predict(x)\n",
    "    plot_prediction(y_true, y_pred)\n",
    "    score = r2_score(y_true, y_pred)\n",
    "    plt.title(f\"R2 = {score}\")\n",
    "\n",
    "    \n",
    "def marginal_prediction_comp_1(mod):\n",
    "    \"\"\"preduce predictions varying just the first feature input\"\"\"\n",
    "    \n",
    "    def f(x):\n",
    "        return np.array([x, 0, 0, 0])\n",
    "\n",
    "    xg = np.linspace(-3,3, 101)\n",
    "    x_comp_1 = np.vstack(f(x) for x in xg)\n",
    "    \n",
    "    return xg, rf.predict(x_comp_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I plot the joint pdf of the prediction value of component 0 vs the actual value. I have used a non-uniform colorbar so we can see both the bulk of the distribution and it's tail. For reference, I have included a line with a slope of 1. Ideally the pdf would tightly cluster around this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(y0, \n",
    "                 LinearRegression().fit(x,y0).predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the linear prediction is not good at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "rf = mem.cache(RandomForestRegressor().fit)(x,y0)\n",
    "rf_pred = rf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(y0, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest seems to do much better, but I am a little concerned about overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_hexbin_matrix(myhexbin, x, rf_pred[:,None]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very similar to the first row of the scatter plot matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the what the output looks like when the components 2-4 are fixed at 0 and the component 1 is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*marginal_prediction_comp_1(rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the fit is pretty noisy, but captures the general features of the data very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyearth import Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code is to slow to use all the samples, so I have to subsample they data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_rand = np.random.choice(x.shape[0], 100000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars = Earth()\n",
    "mars.fit(x[idx_rand], y0[idx_rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(y0, mars.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure if I am tuning this incorrectly somehow, I am a bit surprised the performance is so bad. Actually these errors are from only training on a subset of the data. Here is the error plot for RandomForest trained with just the small subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = mem.cache(RandomForestRegressor().fit)(x[idx_rand],y0[idx_rand])\n",
    "rf_pred = rf.predict(x)\n",
    "plot_performance(y0, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Finally, we get to NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn = MLPRegressor(hidden_layer_sizes=(10,10,10), activation='relu', batch_size=10000)\n",
    "nn = mem.cache(nn.fit)(x, y0)\n",
    "\n",
    "plot_performance(y0, nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN doesn't seem to perform that well even when trained with all the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work better when a larger batch size is used? It seems like the neuralk network is not looking at all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(hidden_layer_sizes=(10,10,10), activation='relu', batch_size=10000, max_iter=10000)\n",
    "nn = mem.cache(nn.fit)(x, y0)\n",
    "\n",
    "plot_performance(y0, nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not seem to be getting better. Can this neural network implementation possibly be looking at all of the samples when it trains so quickly? Part of me suspects that the random forest must be overfitting the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
