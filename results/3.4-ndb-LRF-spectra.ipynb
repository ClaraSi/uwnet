{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrum of Linear Response Functions\n",
    "\n",
    "In this report, I study the properties of the linear fit obtained by MCA regression of the temporally differenced data. As usual $X$ is the data matrix formed by concatenating the vertical profiles the $q_T$ and $s_l$ for each spatial location, $X'$ is the version of this data matrix shifted forward by one time point, and $F$ is the matrix of known source terms (not shifted in time). Also, $W_X$ is a diagonal matrix which weights each vertical location by the square root of its current layers mass and normalizes the different physical variables. The scale of the each variable is defined as the square root of the mass-weighted vertical average of the variance. In other words,\n",
    "$$\n",
    "W_X[i+n_z \\alpha, i+n_z \\alpha] = \\sqrt{\\frac{\\Delta p_i}{\\sigma^2_\\alpha}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\sigma_{\\alpha}^2 = \\sum_{i}^{n_z} \\text{Var}_{x,y,t}(\\alpha) \\Delta p_i.\n",
    "$$\n",
    "\n",
    "\n",
    "Then, the time derivative is approximated by\n",
    "$$ \n",
    "\\frac{X-X'}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "In otherwords, if the loading matrices $\\Phi_X$ and $\\Phi_Y$ are given by $\\Phi_X, \\Phi_Y \\leftarrow MCA(W_X( X'-X), W_X( X+\\Delta t F);n=4)$. Once the MCA loadings are obtained, they are used to compute the MCA scores of the inputs, which are then regressed against the desired outputs $Y=X'-X - \\Delta t F$. This whole procedure specifies the linear model\n",
    "$$\n",
    "X'-(X + \\Delta t F) = M \\Phi_X^T W_X (X+ \\Delta t F) + E.\n",
    "$$\n",
    "This model can be rewritten as $X' = (I+M \\Phi_X^T W_X) (X+ \\Delta t F) + E$. Then, the linear generator of this process is approximated by $A = \\frac{\\log(I+M \\Phi_X^T W_X)}{\\Delta t}$.\n",
    "\n",
    "The variable $\\tilde{X}=X+ \\Delta t F$ has the interpretation of the variable after all known source terms have updated the solution state. This can be interprested as the model state at the end of a climate model's time step. Therefore, predicting $X'$ from $\\tilde{X}$ amounts to predicting the beginning of the next time from the end of the current time step. This formulation allows an easy implementation inside a complicated climate model by monitoring the model state at the end of each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "from lib.lrf import plot_lrf\n",
    "from lib.mca import MCARegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DMD data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = xr.open_dataset(\"../data/raw/ngaqua/stat.nc\").p\n",
    "dmd_data = joblib.load(\"../data/ml/dmd.pkl\")\n",
    "\n",
    "x_train, y_train = dmd_data['train']\n",
    "scale_in, scale_out = dmd_data['scale']\n",
    "weight_in, weight_out = dmd_data['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.cond(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to be infinity, so some sort of regularization is needed. Uncommnent the code cell above if you want to see for your self."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal components analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "ntrain = 400000\n",
    "idx = np.random.choice(x_train.shape[0], ntrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the PCA structure like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(10).fit(x_train[idx]*np.sqrt(weight_in)/scale_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Bars(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first mode is fairly dominant, but the variance explained does not fall off very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCA Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use four MCA components (the default value in our MCARegression code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCA\n",
    "mca = MCARegression(LinearRegression(),\n",
    "                    n_components=5,\n",
    "                    scale=(np.sqrt(weight_in)/scale_in, np.sqrt(weight_out)/scale_out))\n",
    "\n",
    "mca.fit(x_train, y_train-x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract the linear operator $B= I + M \\Phi_X W_X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add identity again\n",
    "I = np.eye(x_train.shape[1])\n",
    "B  = (mca.predict(I) - mca.predict(0*I)) +I\n",
    "\n",
    "hv.Image(B).to.curve(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions help plot the spectra of $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_eig(A):\n",
    "\n",
    "    lam, P = np.linalg.eig(A)\n",
    "    lam_sort = np.abs(lam).argsort()\n",
    "   \n",
    "    lam = lam[lam_sort[::-1]]\n",
    "    P = P[:,lam_sort[::-1]]\n",
    "    \n",
    "    return lam, P\n",
    "\n",
    "def eigvals_plot(lam):\n",
    "    return hv.Points((lam.real, lam.imag)).redim(x=\"Re\", y=\"Im\") * hv.Ellipse(0, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the eigenvalues of $B$ plotted in the complex plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points(size=4)\n",
    "lam, P = sorted_eig(B)\n",
    "eigvals_plot(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues of this matrix are all within the unit circle, which indicates that the linear fit is stable. Now, we can examine the infinitesimal generator $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import logm, expm\n",
    "A = logm(B)/(3/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to figure out why these results are different from the ones obtained in notebook `3.4-ndb-spectra-of-modes.ipynb`. In theory these matrices should be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve {+axiswise}\n",
    "hv.Image(P.real).to.curve(\"x\") + hv.Image(P.imag).to.curve(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These eigenvalues are junk! I am not even sure what the DMD should look like for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot LRF\n",
    "\n",
    "Here is the plot of the generator $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add important index information to the linear response function\n",
    "lrf = pd.DataFrame(A, index=x_train.indexes['features'],\n",
    "                   columns=y_train.indexes['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lrf(lrf, p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Many principal components are needed to compress the input data.\n",
    "- The linear response function obtained by MCA Regression with 4 components is stable\n",
    "- The eigenmodes of the $A$ are unintelligible.\n",
    "\n",
    "\n",
    "I think the best path forward is to use the optimized DMD algorithm which fits for the best modes and eigenvalues simultaneously. Our current approach of finding the MCA modes, fitting a linear response matrix, and then taking its matrix logarithm can introduce significant bias. The oDMD is a state of the art method for performing all these steps simulateneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
