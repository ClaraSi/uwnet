{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import gnl.xarray\n",
    "from gnl.xarray import xcorr, xr2mat\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-presentation of last weeks results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation of precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = xr.open_dataset(\"/home/disk/eos4/nbren12/Data/id/0c31ce4c6763d8ec2abccafe6bde2fa0bed8124d/data/2d/Prec.nc\")\\\n",
    "           .Prec\\\n",
    "           .coarsen(x=64)\\\n",
    "           .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = xcorr(precip, dim='time').mean('x')\n",
    "plt.plot(ac.time * 24, ac)\n",
    "axhline(1/exp(1))\n",
    "xlim([0,24])\n",
    "plt.xlabel(\"lag [hr]\")\n",
    "plt.ylabel(\"Correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precipitation in this 2D dataset decorrelates much more rapidly than in the NGAqua dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ngaqua = \"/home/disk/eos4/nbren12/Prec_coarse.nc\"\n",
    "prec_ngaqua = xr.open_dataset(ngaqua).Prec\n",
    "\n",
    "def f(x):\n",
    "#     print(x)\n",
    "    return xcorr(x, dim='time').mean('x')\n",
    "\n",
    "acf = prec_ngaqua.groupby('y').apply(f)\n",
    "\n",
    "plt.pcolormesh(acf.time*24, acf.y/1e3, acf.T, cmap='viridis')\n",
    "plt.colorbar()\n",
    "lev = 1/exp(1)\n",
    "cs = plt.contour(acf.time*24, acf.y/1e3, acf.T, [lev])\n",
    "\n",
    "plt.text(6.0, 5800, u\"$e^{-1}$\", color=\"w\")\n",
    "\n",
    "xlim([0,20])\n",
    "plt.ylabel('y [km]')\n",
    "plt.xlabel('Lag [hr]')\n",
    "plt.title(u\"Autocorrelation of 160 km averaged precip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelationt time in the NGAqua simulations is a little bit longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 and Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = xr.open_dataset(\"wd/stat.nc\").p\n",
    "q1 = xr.open_dataset(\"wd/calc/q1.nc\")\n",
    "q2 = xr.open_dataset(\"wd/calc/q2.nc\").q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3*1.61))\n",
    "mu = q1.q1.mean(['x','time'])\n",
    "plt.plot(mu, p, label='Q1')\n",
    "\n",
    "mu = q2.mean(['x','time'])\n",
    "plt.plot(mu, p, label='Q2')\n",
    "\n",
    "\n",
    "plt.plot(q1.tend.mean('time'), p, label='$Q_r$')\n",
    "ylim([1000, 10])\n",
    "plt.legend()\n",
    "xlabel(\"K/day\")\n",
    "ylabel(\"p [hPa]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the plots of the domain and time mean Q1 and Q2, where Q1 does not include the radiative component and the large-scale forcings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "Here the standard deviation of Q1 and Q2 for each vertical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3*1.61))\n",
    "mu = q1.q1.std(['x','time'])\n",
    "plt.plot(mu, p, label='Q1')\n",
    "\n",
    "mu = q2.std(['x','time'])\n",
    "plt.plot(mu, p, label='Q2')\n",
    "\n",
    "\n",
    "ylim([1000, 10])\n",
    "plt.legend()\n",
    "xlabel(\"K/day\")\n",
    "ylabel(\"p [hPa]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Covariance Analysis\n",
    "\n",
    "I normalizing using the mass-weighted standard deviation for each variable, because I cannot think of a more parsimonous way to do this. I do not include LHF and SHF in the analysis beacuse it has different physical units than $s_l$ and $q_t$, which can each be put into J/kg. This means that sort of ad-hoc scaling must be used to weight LHF equally to a whole profile of $q_t$ and $s_l$. Since the mass-weighted integrals Q1 and Q2 each of units of W/m2,  the LHF and SHF could be modelled as part of the respones, but this would violate our intuition that LHF can control convection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spec(eig):\n",
    "    plt.bar(np.r_[:20],np.cumsum(eig.values)*100)\n",
    "    plt.xticks(np.r_[:20])\n",
    "    plt.ylabel(\"Cummulative (Co)variance Explained\")\n",
    "    plt.xlabel(\"Number of modes\")\n",
    "\n",
    "def plot_modes(d, modes):\n",
    "\n",
    "    colors = [\"blue\", \"dark red\", \"medium green\", \"dark blue\"]\n",
    "    sns.set_palette(sns.xkcd_palette(colors))\n",
    "\n",
    "    fig, axs = plt.subplots(1,4, sharey=True)\n",
    "\n",
    "    # keep lines for making figure\n",
    "    lines = []\n",
    "    for m in modes:\n",
    "        dm = d.isel(m=m)\n",
    "        \n",
    "        scal = np.sign(float(dm.qt.sel(z=2e3, method='nearest'))) * np.abs(dm.qt).max()\n",
    "\n",
    "        lines.append((axs[0].plot(dm.q1c/scal,d.p)[0], f\"m = {m}\"))\n",
    "        axs[1].plot(dm.q2/scal,d.p)\n",
    "        axs[2].plot(dm.sl/scal,d.p)\n",
    "        axs[3].plot(dm.qt/scal,d.p)\n",
    "        plt.ylim([1000, 10])\n",
    "\n",
    "        for ax, title in zip(axs.flat, [r'$Q_1$ [K/day]', r'$Q_2$ [K/day]', r'$s_l$ [K]', r'$q_t$ [g/kg]']):\n",
    "            ax.set_title(title)\n",
    "\n",
    "\n",
    "    axs[0].set_ylabel('p [hPa]')        \n",
    "    fig.legend(*zip(*lines),loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With phyiscal units the data look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl  =xr.open_dataarray(\"wd/calc/sl.nc\")\n",
    "qt = xr.open_dataarray(\"wd/calc/qt.nc\")\n",
    "D = xr.merge([sl, qt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (a,b, c) = plt.subplots(1,3, sharey=True)\n",
    "\n",
    "\n",
    "sample_dims = ['x', 'time']\n",
    "\n",
    "a.plot(sl.mean(sample_dims), p)\n",
    "a.set_ylim([1000,10])\n",
    "a.set_xlabel('$s_l$ [K]')\n",
    "a.set_ylabel('p [hPa]')\n",
    "\n",
    "\n",
    "b.plot(qt.mean(sample_dims), p)\n",
    "b.set_ylim([1000,10])\n",
    "b.set_xlabel('$q_T$ [g/kg]')\n",
    "\n",
    "c.plot(qt.std(sample_dims), p, label='SD($q_T$) [g/kg]')\n",
    "c.plot(sl.std(sample_dims)/5, p, label='SD(s_l)/5 [K]')\n",
    "c.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the variance of $q_t$ is concentrated in the lower troposphere could potentially mask some important effects. On the other hand, mid-tropospheric moisture is well understood to be an important control on convection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = xr.open_dataset(\"wd/calc/mca.nc\")\n",
    "mca['p'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spec(mca.eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_modes(mca, range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of magnitude of the first three s_l and q_t modes is around 10, which to me says that the mode is more senstive to small changes in $q_t$. This analysis cannot reveal anything about causation. Note that because these are linear modes, the sign and magnitude is arbitrary. In other words, the ratio the same mode between variables is important. I have scaled all the modes so that moisture is postive in the boundary layer, and the maximum magnitude of $q_t$ is set to 1.\n",
    "\n",
    "The moisture and apparent heating modes appear to have the standard baroclinic mode interpretations, whereas the structure of the temperature mode is more complex. This analysis could imply that convection primarily responds to the particular vertical structure of moisture. The basis which captures vertical shifts in the moisture profile the best, is simply a Fourier mode type basis, so I believe that is what we are seeing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Least Squares Analysis\n",
    "\n",
    "PLS is similiar to maximum covariance analysis. If $X$ has size $s \\times  n$ and $Y$ has sign $s \\times m$, then the data can be decomposed \n",
    "$$ X = TP' + E,\\quad Y= UQ' +F.$$\n",
    "To understand the PLS algorithm, we first consider the problem of finding the pair of weights with unit norm $u$ and $v$ which maximize the quantity $cov(Xu, Yv)=u'X'Yv$. These weights are just the left and right eigenvectors of the cross-covariance matrix $X'Y$, which can be found using a power iteration computation. Once the first maximizing weight is found, the matrix is deflated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Homogenous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pls_plots import plot_coef, plot_pls_mode, plot_coef1, pls_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the partial least squares analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_plots(\"wd/calc/pls.nc\", \"wd/stat.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the 2D SAM simulation has fixed radiative cooling, the demeaned $Q_1$ is the same as the demeaned $Q_{1c} = Q_1 -Q_r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGAqua at equator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_plots(\"wd/ngaqua/pls.nc\", \"wd/ngaqua/p.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated Linear Response function\n",
    "\n",
    "This is given by $PT'UQ'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the coefficient matrix for the SAM run. It looks pretty reasonable compared to the linear response functions generated by Zhiming Kuang, but I need to make a more careful comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_coef(\"wd/calc/pls.pkl\", \"wd/w.nc\")\n",
    "ax[0,0].set_xlim([0,15000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the loading matrices $P$ and $Q$ are vey similar between the two datasets. I am getting weird results for the coefficient matrix of the NGAqua data. My guess is that this is caused by using the daily averaged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(\"wd/ngaqua/pls.pkl\", \"wd/ngaqua/w.nc\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalization approaches\n",
    "In general, MCA can be viewed as basically a regularization approach to fitting a linear model \n",
    "$$Y = XB$$\n",
    "where $Y$ is a $s\\times n$ matrix and $X$ is an $s\\times m$ matrix. $s$ is the number of samples (e.g. grid boxes and time points) while $m$ and $n$ are the number of feature in the output and input respectively. For example, the features of the input in the cumulus parametrization problem are the vertical profiles of $s_l$ and $q_t$ evaulated at each vertical grid point. For the 2D SAM dataset this implies that $m=n=n_z\\times 2=128$ data points.\n",
    "\n",
    "The standard way to fit a multivariate linear regression problem is to perform a least squares fit. However, when performing a linear fit, the condition number of the problem should be small. Is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = xr.open_mfdataset([\"wd/calc/sl.nc\", \"wd/calc/qt.nc\", \"wd/calc/q1.nc\", \"wd/calc/q2.nc\"])\n",
    "w = xr.open_dataarray(\"wd/w.nc\")\n",
    "\n",
    "D = D * w\n",
    "X,scalex = xr2mat(D[['qt', 'sl']], ['x', 'time'], ['z'], scale=True)\n",
    "\n",
    "# perform computation\n",
    "X.load()\n",
    "\n",
    "# compute condition number\n",
    "np.linalg.cond(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the condition number of this matrix is extremely high, so using ordinary least squares to find the matrix $B$ is a hopeless task. The reason the matrix $X$ is ill-condition is beacause of collinearity in the input variables. For example, $s_l$ and $q_t$ at neighboring heights will be highly correlated.\n",
    "\n",
    "This ill conditioning is very common in multivariate problems like this, and there are a variety of popular techinques that are used to \"regularize\" the problem. Some examples of these techniques include\n",
    "\n",
    "1. Penalization methods like Ridge Regression (L2 penalty) and the LASSO (L1 penalty). These methods penalize large coefficent values, and the LASSO promotes sparsity. Another simple idea, which would promote smooth solutions would be to the 2nd vertical derivative of the fitted coefficients.\n",
    "2. Partial least squares techniques. This class of problems includes Maximum Covariance Analysis (MCA) as well as the namesake Partial Least Squares regression technique. These methods essentially project both the inputs and outputs onto truncated sets of orthogonal modes which covary strongly. This method improves the conditionining of the problem by projecting the data to a particularly efficient basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step towards finding the whole linear response function, we attempt to find a linear model for the precipitation, which is related to the vertical integral of the first mode we found in the MCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and subsample precipitation dataset\n",
    "prec = xr.open_dataarray(\"wd/A64/2d/Prec.nc\").reindex_like(D)\n",
    "\n",
    "y = xr2mat([prec], ['x','time'], [], scale=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, precipitation is very fat-tailed as can be seen in the following log-pdf plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnl.plots import loghist\n",
    "\n",
    "loghist(prec.values.ravel())\n",
    "ylim([-10,0])\n",
    "legend([\"log-pdf of precip\", \"log-pdf of gaussian\"])\n",
    "xlabel(\"mm/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because precipitation is very fat-tailed we transform it using the logarithm  so that \n",
    "$$y = \\log(P + .1) $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = .1\n",
    "yt = np.log(y + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting histogram plot looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loghist(yt.values.ravel())\n",
    "legend([\"log-pdf of y\", \"gaussian comparison\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is clearly not normalized, but it is better than before. Now, let's try to fit a ridge regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge regression is given by the following penalized least-squares problem \n",
    "$$\\min_{B} \\frac{1}{2}|Y - XB|^2 + \\alpha |B|^2.$$\n",
    "The parameter $\\alpha$ is used to penalize large values of the estimated coefficients $B$.\n",
    "\n",
    "In terms of physical quantities, the minimization problem I aim to solve is \n",
    "$$\\min_{a,b} \\frac{1}{2}\\left|\\tilde{P} + a(z)\\tilde{s_l}(z)  + b(z) \\tilde{q_t}(z)\\right|^2 + \\alpha \\int \\left[a(z)^2   + b(z)^2 \\right] \\rho_0 dz$$.\n",
    "Where $\\tilde{\\cdot}$ is the normalization operator. The profiles $q_t$ and $s_l$ are normalized by the square root of the vertically integrated variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ridge(alpha):\n",
    "    mod = Ridge(alpha=alpha, normalize=False)\n",
    "    mod.fit(X-X.mean('samples'), yt)\n",
    "    \n",
    "    Bqt, Bsl = np.split(mod.coef_.ravel(), 2)\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(Bqt/w, p)\n",
    "    plt.xlabel(r\"$q_l$\")\n",
    "    plt.ylim([1000,10])\n",
    "    plt.ylabel('p [hPa]')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(Bsl/w, p)\n",
    "    plt.xlabel(r\"$s_l$\")\n",
    "    plt.ylim([1000,10])\n",
    "    \n",
    "    plt.suptitle(f\"Coefficients for alpha = {alpha}\")\n",
    "    \n",
    "    # plot precip output\n",
    "    plt.figure(figsize=(6/1.61,6))\n",
    "    pred = np.exp(xr.DataArray(mod.predict(X-X.mean('samples')), yt.coords).unstack('samples'))-eps\n",
    "    pred.plot(vmin=0, x='x', y='time')\n",
    "    plt.title(f\"Predicted Precipitation for alpha = {alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for small $\\alpha$ the solutions are not very stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ridge(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But increasing $\\alpha$ leads to much more stable answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ridge(1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard practice in machine learning is to choose parameters like $\\alpha$ can be chosen by a K-fold cross-validation. I would recommend using the relative entropy between the distributions of the actual and predicted precipitation as a measure of the performance of given set of hyper parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
