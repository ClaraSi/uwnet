{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using are only evaulated with a coarse sampling time step of 3 hours. On the other hand, we will probably use 10-20 minute time step for the coarse resolution model. This means that the dynamical model we are trying to fit is \n",
    "$$ x^i_{n+1} = \\underbrace{f(f(\\ldots f}_{\\text{m times}}(x^i_n))) + \\int_{t_n}^{t_{n+1}} g(x(t), t) dt$$ \n",
    "where $i$ is the horizontal spatial index, and $n$ is the time step. The number of times the function $f$ is applied is $m=\\frac{\\Delta t}{h}$ where $h$ is the GCMs time step, and $\\Delta t$ is the sampling interval of the stored output. The integral on the right represents the approximately known terms such as advection, and $f$ represents the unknown source terms.\n",
    "\n",
    "We solve a minimization problem to find $f$. This is given by \n",
    "$$\n",
    "\\min_{a} \\lim_{m \\rightarrow \\infty} \\sum_{i,n} ||x^{i}_{n+1} - F^{(m)} x^i_{n} - g_n^{i}||_W^2 \\quad \\text{s.t.}\\quad F^{(m)}(\\cdot) = \\underbrace{f(f(\\ldots f}_{\\text{m times}}(\\cdot))),\\ f(x) = x +  \\frac{ \\Delta t}{m} a(x).\n",
    "$$\n",
    "Intuitively, the forward operator $F^{(m)}$ is the result applying $m$ forward euler steps to the system $a$.\n",
    "\n",
    "Let's try performing this fit. First, we need to import the appropriate models, and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import attr\n",
    "\n",
    "from xnoah.data_matrix import stack_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "w = xr.open_dataset(\"../data/processed/ngaqua/w.nc\")\n",
    "# data\n",
    "X = xr.open_mfdataset(\"../data/calc/ngaqua/*.nc\")[['qt', 'sl']].load()\n",
    "G = xr.open_mfdataset(\"../data/calc/adv/ngaqua/*.nc\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the coarse sampling time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = float(X.time[1] - X.time[0])*86400\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many 10 minute time steps are in this period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt//(60*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a torch module for the function $a$. It will just be a single layer perceptron, which appropriately scales the inputs first. Let's first compute the appropriate scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepvar(X):\n",
    "    return stack_cat(X, \"feature\", [\"z\"])\n",
    "\n",
    "def prep_for_torch(X):\n",
    "    x =  prepvar(X).pipe(np.asarray)\n",
    "    return Variable(torch.FloatTensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledSLP(nn.Module):\n",
    "    \"\"\"Scaled single layer preceptron\"\"\"\n",
    "    \n",
    "    def __init__(self, mu, sig):\n",
    "        super(ScaledSLP, self).__init__()\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.sig = sig\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(68, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 68)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.sub(self.mu).mul(1/(self.sig + 1e-5))\n",
    "        return self.net.forward(x)\n",
    "        \n",
    "        \n",
    "class EulerStepper(nn.Module):\n",
    "    \"\"\"time stepping class\"\"\"\n",
    "    def __init__(self, net, n, h):\n",
    "        super(EulerStepper, self).__init__()\n",
    "        self.net = net\n",
    "        self.n = n\n",
    "        self.h = h\n",
    "        \n",
    "        # The default weight initialization are too large,\n",
    "        # and yield unreasonably large values applied several times \n",
    "        # when \n",
    "        # for now let's just use 0.0 as an initial condition\n",
    "        # this should probably be changed to random initalization\n",
    "        # with very small weights.\n",
    "        for mod in self.modules():\n",
    "            if isinstance(mod, nn.Linear):\n",
    "                mod.weight.data.fill_(0.0)\n",
    "                mod.bias.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n):\n",
    "            x =  x +  self.net(x).mul(self.h)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we apply this once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = prep_for_torch(X.mean(['x', 'y', 'time']))\n",
    "sig = prep_for_torch(X.std(['x', 'y', 'time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ScaledSLP(mu, sig)\n",
    "stepper = EulerStepper(a, n=18, h=dt/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prep_for_torch(X.isel(x=0,y=0,time=0))\n",
    "stepper.forward(x)[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to work. Now let's work on fitting this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define some utilities for loading the data from xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_var(X):\n",
    "    return prepvar(X).stack(samples=['x', 'y', 'time']).transpose('samples', 'feature')\n",
    "\n",
    "\n",
    "class ParamDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, G):\n",
    "        batch_dims = ['x', 'y', 'time']\n",
    "        \n",
    "        # forcing and initial condition data\n",
    "        self.x_stacked = get_stacked_var(X.isel(time=slice(0, -1)))\n",
    "        self.g_stacked = get_stacked_var(G.isel(time=slice(0,-1)))\n",
    "        \n",
    "        # data shifted forward in time\n",
    "        self.xp_stacked = get_stacked_var(X.isel(time=slice(1, None)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_stacked)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return [np.asarray(x) for x in\n",
    "        [self.x_stacked[idx], self.xp_stacked[idx], self.g_stacked[idx]]]\n",
    "\n",
    "\n",
    "train_dataset = ParamDataset(X.isel(y=slice(32-3,32+3)), G.isel(y=slice(32-3,32+3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass this dataset object to `DataLoader`, which will generate the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO replace these functions\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "data_dict = joblib.load(\"../data/ml/ngaqua/data.pkl\")\n",
    "w = data_dict['w'][1]\n",
    "scale = data_dict['scale'][1]\n",
    "w_torch = Variable(torch.FloatTensor(w/scale**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y, output, g):\n",
    "    return torch.mean(torch.pow(output + g.mul(dt)-y, 2).mul(w_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "Now, let's train the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "# make the optimizer function\n",
    "optimizer = torch.optim.Adam(stepper.parameters(), lr=.001)\n",
    "\n",
    "# Do the time stepping\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    for batch_idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        x,y,g = map(Variable, batch)\n",
    "        optimizer.zero_grad()  # this is not done automatically\n",
    "        pred = stepper(x)\n",
    "        loss = loss_function(y, pred, g)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.data.numpy()\n",
    "\n",
    "    print(f\"Epoch: {epoch} [{batch_idx}]\\tLoss: {avg_loss}\")\n",
    "    avg_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = prep_for_torch(X.isel(y=32, x=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(a(t).data.numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training basically failed. This idea of using multiple time steps is very expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
