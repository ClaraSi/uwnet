{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from lib.util import output_to_xr, dict_to_xr\n",
    "from lib.torch_models import TorchRegressor, single_layer_perceptron, residual_net\n",
    "from lib.plots.model_evaluation import scatter_plot_z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open NGaqua data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the loss, which should be mass-weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"../data/ml/ngaqua/data.pkl\")\n",
    "\n",
    "# load weight data\n",
    "_, w = data['w']\n",
    "w = Variable(torch.FloatTensor(w))\n",
    "\n",
    "# mass weighted loss\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def loss_function(output, y):\n",
    "    return mse(output.mul(w.sqrt()), y.mul(w.sqrt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data['train']\n",
    "\n",
    "inds = np.random.choice(x_train.shape[0], 200000)\n",
    "x_train, y_train = x_train[inds], y_train[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp = TorchRegressor(net_fn=single_layer_perceptron,\n",
    "               loss_fn=loss_function,\n",
    "               optim_kwargs={'lr': .001},\n",
    "                    num_epochs=2)\n",
    "\n",
    "slp = make_pipeline(VarianceThreshold(.001), StandardScaler(), slp)\n",
    "\n",
    "slp.fit(x_train, y_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnet = TorchRegressor(net_fn=residual_net,\n",
    "               loss_fn=loss_function,\n",
    "               optim_kwargs={'lr': .001},\n",
    "                     num_epochs=2)\n",
    "\n",
    "rnet = make_pipeline(VarianceThreshold(.001), StandardScaler(), rnet)\n",
    "rnet.fit(x_train, y_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = make_pipeline(VarianceThreshold(.001), StandardScaler(), LinearRegression())\n",
    "lm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x, y = data['test']\n",
    "\n",
    "predictions = {'true': y, 'rnet': rnet.predict(x), 'slp': slp.predict(x), 'linear': lm.predict(x)}\n",
    "predictions = {k: output_to_xr(v, y.coords) for k, v in predictions.items()}\n",
    "preds_xr = dict_to_xr(predictions, dim_name=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the predicted value of $Q1_c$ compared to the measured value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image[width=800, height=160, colorbar=True](cmap='inferno')\n",
    "\n",
    "hv.Dataset(preds_xr.isel(x=0, y=8).Q1c).to.image([\"time\", \"z\"])\\\n",
    ".layout().cols(1)\\\n",
    ".redim.range(z=(0,20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some scatter plots of the predicted vs the actual Q1c for different heights. The x and y axes are scaled so that 0 (1) corresponds to the minumum (maximum) actual Q1c for that height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scatter_plot_z(preds_xr.Q1c, \"slp\", \"true\", \"model\").relabel(\"RNet\")\n",
    "+scatter_plot_z(preds_xr.Q1c, \"rnet\", \"true\", \"model\").relabel(\"RNet\")\n",
    "+ scatter_plot_z(preds_xr.Q1c, \"linear\", \"true\", \"model\").relabel(\"Linear Model\")).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNet and the modified single layer perceptron (SLP) both perform similarly well on this training dataset, but the linear model is much worse. In fact, the neural network fitted models, almost seem like a denoised version of the estimated heat source. Basically, it seems the Krasnopolsky result is the best option as far as deterministic parametrization is concerned. The main questions I have now are:\n",
    "\n",
    "\n",
    "- Will these schemes perform well in a prognostic setting?\n",
    "- Can we train these networks on the full NGAqua dataset including the tropics and sub-tropics? If not, what sort of network architectures will work well for that dataset?\n",
    "- Are the answers very different when we train on a simulation with a different climate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uw-machine-learning]",
   "language": "python",
   "name": "conda-env-uw-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
