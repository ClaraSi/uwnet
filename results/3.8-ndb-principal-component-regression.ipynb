{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks shows results for principal components regression of the NGAqua data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from toolz import *\n",
    "from toolz.curried import get\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.externals import joblib\n",
    "from lib.models import get_pcr_mod, mse, weighted_r2_score\n",
    "from lib.util import dict_to_xr\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"../data/ml/ngaqua/data.pkl\")\n",
    "ntrain = 10000\n",
    "\n",
    "_, weight_out = data['w']\n",
    "\n",
    "x_train, y_train = data['train']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "# training indices (random sample for speed)\n",
    "train_inds = np.random.choice(x_train.shape[0], ntrain, replace=False)\n",
    "\n",
    "\n",
    "def score_model(mod, x, y):\n",
    "    pred = mod.predict(x)\n",
    "    return weighted_r2_score(y, pred, weight_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the PCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr = get_pcr_mod(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_data = {}\n",
    "\n",
    "\n",
    "for n in tqdm([1,2,5,10,20,30,40,50,60,68]):\n",
    "#     print(f\"Fitting model for {n} components\")\n",
    "    # set the number of components to keep\n",
    "    pcr.pca.set_params(n_components=n)\n",
    "    pcr.fit(x_train[train_inds], y_train[train_inds])\n",
    "    cross_val_mse = mse(pcr.predict(x_test), y_test, dims=['samples'])\n",
    "    \n",
    "    cv_data[n] = {'test_score': score_model(pcr, x_test, y_test),\n",
    "                 'train_score': score_model(pcr, x_train, y_train),\n",
    "                 'mse_profile': cross_val_mse}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts NdOverlay[legend_position='top_left']\n",
    "df = pd.DataFrame({'test': valmap(get('test_score'), cv_data),\n",
    "                   'train': valmap(get('train_score'), cv_data)}).reset_index()\n",
    "hv.Table(pd.melt(df, id_vars='index')).to.curve(\"index\", \"value\").overlay()\\\n",
    ".redim.range(value=(0,.4))\\\n",
    ".redim.label(index=\"n\", value=\"R2 score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCR performs poorly for small numbers of retained components. What is the variance explained of the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Curve(pcr.pca.explained_variance_ratio_.cumsum(), kdims=['n'], vdims=['Cumulative fraction of explained variance'])\\\n",
    ".redim.range(n=(0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few modes, explain much of the variance, so as with MCA based regression, they main problem is likely the nonlinearity between the modes rather than the importance of extremely low variance modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what do the vertical structures of the errors look like? To do this, let's first collect all the MSE data into one data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_mse = valmap(get('mse_profile'), cv_data)\n",
    "\n",
    "mse_data = xr.concat(cross_val_mse.values(), dim=pd.Index(cross_val_mse.keys(), name='n')).unstack(\"features\")\n",
    "mse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I overlay all the different curves in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve[invert_axes=True] NdOverlay[show_legend=False]\n",
    "m = hv.Dataset(mse_data.sel(n=slice(0,40))).to.curve(\"z\").overlay(\"n\").layout(\"variable\").redim.label(Q1c=\"MSE\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the errors are larger for Q1c than they are for Q2. It is interesting that including more components decreases the error in the troposphere,but does .not decrease the error for Q1c in the stratosphere. This indicates that the errors there are inherently unpreditable there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uw-machine-learning]",
   "language": "python",
   "name": "conda-env-uw-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
