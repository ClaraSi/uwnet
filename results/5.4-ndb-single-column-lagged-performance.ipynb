{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged predictions\n",
    "\n",
    "In this notebook, we plot the output of the time stepping scheme over varius prediction intervals, and compare it to the truth. In particular, define the lagged prediction of a given time point be given by\n",
    "$$ \\tilde{x}^n_m = f^m(x^{n-m}),$$\n",
    "where $f^m$ is the time stepping operator defined recursively by\n",
    "    $$f^m(x) = f(f^{m-1}(x)) + \\frac{\\Delta t}{2}\\left(g^{m-1} + g^{m}\\right)$$\n",
    "and $f^0(x) = x$. In the formula above, $g^m$ is the advection forcing evaluated at time step $m$.\n",
    "\n",
    "The neural network or other machine learning models gives the function $f$ for the unforced variability. The minimization problem used to find $f$ is\n",
    "$$ \\text{argmin}_{f} \\sum_{i,n} \\sum_{m=0}^{\\ell} ||f^m(x_i^{n-m}) - x_i^n||^2_w.$$\n",
    "Here $i$ is the index of the horizontal spatial location and $n$ is the time index.\n",
    "\n",
    "This minimization problem tries to more directly assess the error of running the scheme in a prognostic mode for $\\ell$ time steps. I have done numerical experiments showing that using $\\ell=2$ tends to give an unstable scheme. Indeed, for $\\ell=2$ the loss function is proportional to the error in predicting the discrete time derivative, which is what we were working on before. In previous reports, I stabilized that scheme by adding an L2 penalization to the weights of the network, but it turns out that we do not need to do this when $\\ell=10$. This result is analogous to the result that accuracy in numerical ODE requires both consistency and stability. When $\\ell=2$ the objective function above only cares about consistency, but with $\\ell=10$ a stable scheme is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.evaluation.single_column import  xr_runsteps, lagged_predictions\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data for a given horizontal location on the equator. We also load the trained neural network predictor $f_{NN}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(x):\n",
    "    return x.isel(x=0, y=32)\n",
    "\n",
    "inputs = xr.open_mfdataset(\"../data/calc/ngaqua/*.nc\", preprocess=subset)\n",
    "forcings = xr.open_mfdataset(\"../data/calc/forcing/ngaqua/*.nc\", preprocess=subset)\n",
    "w = xr.open_dataarray(\"../data/processed/ngaqua/w.nc\")\n",
    "stepper = torch.load(\"../data/ml/ngaqua/multistep_objective.torch\")\n",
    "\n",
    "p = xr.open_dataset(\"../data/raw/ngaqua/stat.nc\").p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I calculate $\\tilde{x}_m^n$ using $f_{NN}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_preds = lagged_predictions(stepper, inputs, forcings, w, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the fields at lag 10\n",
    "\n",
    "Here, I compare $x^n$ with $\\tilde{x}^{n}_{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(lagged_preds,lag=10, **kwargs):\n",
    "\n",
    "    fig, axs = plt.subplots(3,1, sharex=True, sharey=True, figsize=(10,6))\n",
    "\n",
    "    def plot_qt(ax, x):   \n",
    "        im = ax.contourf(x.time.values, p.values, x.T, cmap='viridis', **kwargs)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        return im\n",
    "    \n",
    "    \n",
    "    def add_label(ax, text):\n",
    "        ax.text(105, 200, text, bbox=dict(color='white'))\n",
    "\n",
    "    plot_qt(axs[0], lagged_preds.isel(lag=0))\n",
    "    plot_qt(axs[1], lagged_preds.isel(lag=lag))\n",
    "\n",
    "    err = lagged_preds.isel(lag=0) - lagged_preds.isel(lag=lag)\n",
    "    im = axs[2].pcolormesh(err.time.values, p.values, err.T)\n",
    "    plt.colorbar(im, ax=axs[2])\n",
    "\n",
    "    axs[0].invert_yaxis()\n",
    "    axs[-1].set_xlabel('time [d]')\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_ylabel('p [mb]')\n",
    "        \n",
    "    add_label(axs[0], 'Truth')\n",
    "    add_label(axs[1], 'Lag %d'%lag)\n",
    "    add_label(axs[2], 'Difference')\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plot for the humidity field $q_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(lagged_preds.qt, levels=np.arange(11)*2)\n",
    "plt.xlim([100,130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for the temperature variable $s_L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(lagged_preds.sl-lagged_preds.sl.mean('time'), levels=[-5,-4,-3,-2,-1,0,1,2,3,4,5])\n",
    "plt.ylim([1000,150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertically averaged MSE plots\n",
    "\n",
    "Now, I summarize the difference using the mass-weighted mean square error averaged for each prediction lag over all time points. This quantity is given by the formula\n",
    "$$ \\sum_n ||x^n - \\tilde{x}^n_m ||^2_w.$$\n",
    "\n",
    "I plot this quantity for each physical variable ($q_T$, and $s_L$) separately. I also plot a horizontal dashed line indicating error of the time mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_weighted(true, pred, w=w):\n",
    "    return (((pred-true)**2) * w).sum('z')/w.sum('z')\n",
    "\n",
    "lag_errors = mse_weighted(inputs, lagged_preds).mean('time')\n",
    "mean_error = mse_weighted(inputs, inputs.mean('time')).mean('time')\n",
    "\n",
    "\n",
    "# this closure will be helpful\n",
    "def plot_err(key):\n",
    "    lag_errors[key].plot()\n",
    "    plt.axhline(mean_error[key], c='k', ls='--')\n",
    "    plt.title(key)\n",
    "    plt.xlim([0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_err('sl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_err('qt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the moisture with much more accuracy than the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical structures in Error\n",
    "\n",
    "What does the vertical structure of this error look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_over_time = ((lagged_preds-inputs)**2).mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the error for $q_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(mse_over_time.lag, p, mse_over_time.qt.T, 11)\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('p [mb]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for $s_L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mc\n",
    "\n",
    "plt.contourf(mse_over_time.lag, p, mse_over_time.sl.T, np.arange(11)*.5)\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('lag [d]')\n",
    "plt.ylabel('p [mb]')\n",
    "plt.ylim([1000, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this last plot has a logarithmic colorbar. I had to use this because the error at the tropopause (near 100 mb) is so large compared to the errors elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting $s_l$ errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the time series RMS plot again, but this time we will exclude the large errors in the stratosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lagged_preds.assign(p=p, w=w).sel(z=slice(0,10e3))\n",
    "\n",
    "mse = mse_weighted(data, data.isel(lag=0), data.w).mean('time')\n",
    "mse_mean = mse_weighted(data.isel(lag=0).mean('time'), data.isel(lag=0)).mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.sl.plot()\n",
    "plt.axhline(mse_mean.sl, c='k', ls='--')\n",
    "plt.xlim([0,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance does not improve much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. Using the multiple time step objective function gives more stable results than 1 step objective function.\n",
    "2. The neural network scheme works much better for the moisture than it does for the temperature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
